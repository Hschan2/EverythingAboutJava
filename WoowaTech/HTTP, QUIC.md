# HTTP 1.1, HTTP2, QUIC

## 성능 하락의 큰 요인
* Bandwidth
    * 데이터를 많이 주고받기 위해서 큰 대역폭이 있어야 하는 것이다?
    * 대역폭이 크면 데이터 전송 속도가 빨라진다?

* Latency
    * 데이터 주고 받는데에는 당연이 딜레이가 없어야 한다?
    * 대역폭이 크면 소용 없다. 네트워크 한 방이면 완료가 된다?   

=> 조사 결과, Bandwidth보다는 Latency를 줄이는 방향이 성능 향상에 더 중요하다는 사실을 볼 수 있다.   

## TCP
HTTP는 TCP위에 위치하고 있다. TCP는 무엇일까?   

* Transmission Control Protocol
* 신뢰성을 가지는 프로토콜
* IP 위에서 동작
* 3-Way Handshake (클라이언트랑 서버가 상호간에 데이터를 주고 받을 준비가 되었다는 합의 과정, Syn | Syn/Ack | Ack)   

### 3-Way Handshake
HTTP 1.0을 이용할 시, 문제가 발생한다.   

HTTP 1.0은 한 번에 요청이 있을 때마다 한 번에 커넥션을 만들어야 한다. 따라서 여러 개의 리소스를 요청하기 위해서 여러 번의 커넥션을 생성하고 끊어야 한다. 이러한 과정을 연결 수립 과정으로 인한 Latency를 만든다.   

또한, 데이터 전송에 있어서 속도가 느리다. 느린 전송은 커넥션이 전송할 수 있는 최대 데이터의 크기 한계를 두고 데이터 송수신을 진행하면서 한 번에 보낼 수 있는 데이터 최대 크기를 찾아가는 과정을 말한다. 즉, 크기 제한이 있다.   

이러한 문제를 해결하기 위해 TCP 커넥션 재활용하는 방법을 찾아 사용한다. 재활용을 한다면 Handshake 문제가 사라지고 느린 연결로 인한 문제 또한 사라진다.   

## HTTP 1.1
이 후에 HTTP 1.1이 나오게 되었다. 지속 커넥션을 활성화 시킥 ㅣ대문에 TCP 커넥션 재활용을 하는 이점을 갖게 되었다. 기존 통신 방법과 다르게 3-Way Handshake에서 발생하는 비용을 감소시켰다.   

그러나 HTTP는 전송과 응답의 순서가 같아야 한다. 즉, 클라이언트가 A와 B라는 요청을 서버에 보내면 A와 B의 순서대로 응답해야 한다는 것이다.   

순서대로 응답하지 않는다면 문제가 발생하는 것을 알 수 있다. HTTP 1.1은 데이터를 정기적으로 가져올 수 밖에 없게 된다. 그리고 이는 데이터 요청에 Latency로 이어지게 된다.   

그것이 바로 <b>파이프라인</b>이다.   

해당 문제를 해결하기 위해 FIFO 방법을 사용했다. A를 요청하고 A가 응답이 오면 B를 요청하고 B의 응답을 받는 방법이다.   

이를 서버로 옮겨서 실행을 한다. 요청을 시작하면 서버는 요청에 대한 처리를 하고 처리를 순서에 상관없이 요청의 순서대로 클라이언트에 응답을 한다. 예를 들어, A를 요청하고 B를 요청하는데 B가 먼저 끝나면 A가 끝날 때까지 기다렸다가 A가 응답을 하면 B도 응답을 하게 된다. 이를 <b>파이프라이닝</b>이라는 기술이다.   

그러나 이 또한 문제가 있다. A 요청을 하고 B 요청을 했을 때, B에 대한 작업이 끝나도 A 작업이 끝날 때까지 기다려야 한다. A 작업이 길어진다면 B의 응답은 계속 기다려야 한다. 만약 다음 요청이 있더라도 A에 대한 요청의 작업이 끝나지 않았기 때문에 문제가 발생한다. 이를 <b>Head of Line 블록킹</b>이라고 한다.   

즉, HTTP 1.1은   

* Head of Line Blocking 문제
* 서버에서 병렬 처리 시 응답을 메모리에 적재하고 있어야 하는 문제. 즉, 리소스 낭비
* 응답 실패 시 클라이언트가 모든 리소스에 대한 요청을 처음부터 다시 보내야 하는 문제
* 중계자 존재 시 파이프라인 호환성 문제
* 등등   

이러한 문제를 해결하기 위해 TCP 커넥션을 여러 개 생성하는 방법을 고안한다. 이를 통해서 <b>멀티 플렉싱</b>을 흉내낼 수 있게 되었다. 즉, 데이터를 병렬적으로 받을 수 있게 되었다. (실제로 속도가 빠른 것은 아니지만 여러 개의 리소스를 전송, 응답받을 수 있다는 것에 더 빠르다고 느낀다고 한다.)   

그러나 다수의 커넥션을 사용하면서 클라이언트와 서버에 오버헤드가 발생하는 문제가 존재 했다.   

* 멀티플렉싱: 여러 개의 TCP 연결을 만들어내는 것이 아니라 단일 연결 안에서 여러 개의 데이터가 섞이지 않게 보내는 기법   

이러한 문제들을 보완하기 위해 HTTP 2.0에 스트림 개념을 가져오게 되었다.

## HTTP2
스트림이란 전송되는 데이터에 특별한 식별지를 붙여주는 것이라고 이해하면 쉽다. 이로써 하나의 TCP를 통해 여러 개의 데이터를 병렬적으로 처리할 수 있게 되었다.   

그러나 TCP 프로토콜 자체에 존재하는 Head of Line Blocking 문제는 여전히 존재했다. (HTTP2에 계층이 서로 다른 HTTP 응답 요청, 교환을 별개의 스트림으르 분리했다.)   

해당 문제를 해결하기 위해 구글에서 QUIC를 개발하였다.   

## QUIC
* UDP 사용 (비신뢰성)
* 애플리케이션 계층에 신뢰성 구현   

UDP를 애플리케이션 계층에서 다시 구현함으로서 신뢰성을 갖도록 만들었다.   

Head of Line Blocking 문제를 해결하기 위해 <b>독립 스트림</b>이라는 것을 만든다.   

독립 스트림을 사용함으로서 하나의 스트림에 문제가 생겨도 다른 스트림에 영향을 끼치지 않는 특징을 가지고 있다. (TCP는 하나의 스트림)   

QUIC는 Latency를 줄이면서 성능 향상을 만들어냈다.

[HTTP 1.0, HTTP 1.1, HTTP2, QUIC](https://www.youtube.com/watch?v=ZgSC5K1sUYM)


